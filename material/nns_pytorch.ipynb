{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bit of a hack in case your IDE wants to run the notebook from `/material/` and not the project root folder `/ma1`. We need the working directory to be `/ma1` for local imports to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the working directory is set to the \"ma1\" folder.\n",
    "while Path.cwd().name != \"ma1\" and \"ma1\" in str(Path.cwd()):\n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional machine learning methods, like logistic regression, are often sufficient for datasets that are linearly separable. However, more complex problems, such as image recognition and natural language understanding/processing (NLU/P), sometimes demand a more intricate approach. Neural networks, which incorporate additional layers, excel at learning non-linear relationships. These extra layers, known as *hidden* layers, process the input into one or more intermediate forms before generating the final prediction.\n",
    "\n",
    "Logistic regression achieves this transformation using a single fully-connected layer. We can think of this as a Single-Layer Perceptron (although, please don't call it that). This layer performs a linear transformation (a matrix multiplication combined with a bias). In contrast, a neural network with multiple connected layers is typically referred to as a Multi-Layer Perceptron (MLP). For instance, in the simple MLP shown below, a 4-dimensional input is mapped to a 5-dimensional hidden representation, which is subsequently transformed into a single output used for prediction. This is a \"simple\" architecture: A so-called artificial neural network. \n",
    "\n",
    "<img src=\"../media/MLP.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "#### Nonlinearities revisited\n",
    "\n",
    "Nonlinearities are usually applied between the layers of a neural network. As discussed in class 2, there are several reasons for this. A key reason is that without any nonlinearity, a sequence of linear transformations (fully connected layers) reduces to a single linear transformation, limiting the model's expressiveness to that of a single layer. Including nonlinearities between layers prevents this reduction, enabling neural networks to approximate far more complex functions. This is what makes neural networks so powerful.\n",
    "\n",
    "Numerous nonlinear activation functions are frequently employed in neural networks, but one of the most commonly used is the [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)):\n",
    "\n",
    "```math\n",
    "\\begin{align}\n",
    "x = \\max(0,x)\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "#### Why neural networks?\n",
    "The real power of neural networks lies in their ability to deal with problems that arenâ€™t simple. A lot of traditional methods, like logistic regression, are limited to drawing straight lines (linear decision boundaries) to separate data. That works well if your data happens to fall neatly into two groups, like the example on the left below. But in real life, things are rarely that simple.\n",
    "\n",
    "<img src=\"../media/nonlinearity.png\" width=\"500\"/>\n",
    "\n",
    "Take the example on the right. The data - 2D for simplicity, although most datasets will have many more features - isnâ€™t linearly separable - you canâ€™t just draw one straight line and call it a day. Neural networks solve this by stacking layers with nonlinear activation functions, allowing them to reshape the input data into something more manageable. They essentially learn to create these complex boundaries (like the curved line on the right) that let us separate data no matter how messy it is. This ability to handle non-linearity is what makes neural networks so effective for tasks like recognizing images, processing speech, or understanding language.\n",
    "\n",
    "***\n",
    "\n",
    "We will cover the workings of neural network in class and in the readings. The purpose of this notebook is to provide a hands-on introduction to neural networks using PyTorch. We will\n",
    "\n",
    "* Walk through the components of a neural network in PyTorch\n",
    "* build a simple neural network to classify images from the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and neurons\n",
    "\n",
    "Neurons are individual units in a neural network, pseudo-seriously modelled after how neurons share \"information\" through electric signals in a  brain. They receive input from the data (or neurons in a prior layer) and apply a transformation to it. The output of this transformation is then passed to the next layer of neurons. Think of neurons as individual computational units that process information. Together, they form a network that can learn complex patterns in the data.\n",
    "\n",
    "<img src=\"../media/ann-brain.webp\" width=\"500\"/>\n",
    "\n",
    "In PyTorch, we can create a fully connected layer using `torch.nn.Linear` class. This class has two parameters: the number of input units and the number of output units. The input units are the number of features in the input data, and the output units are the number of neurons in the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: A Single Linear Layer in PyTorch\n",
    "\n",
    "In this example, we use a single linear layer to demonstrate the basic operation of a fully connected layer in PyTorch. A linear layer performs a simple transformation of the input data using the formula:\n",
    "\n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(x\\) is the input tensor,\n",
    "- \\(W\\) is the weight matrix,\n",
    "- \\(b\\) is the bias vector, and\n",
    "- \\(y\\) is the output tensor.\n",
    "\n",
    "\n",
    "The layer is defined using `nn.Linear(input_features, output_features)`. In this case, we map 4 input features to 5 output features. The layer includes both weights and biases, which are automatically initialized by PyTorch. The output is computed by applying the linear transformation to the input tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a single linear layer\n",
    "linear_layer = nn.Linear(4, 5)  # 4 input neurons to 5 output neurons\n",
    "\n",
    "# Example input\n",
    "x = torch.randn(3, 4)  # A batch of 3 examples, each with 4 input features\n",
    "\n",
    "# Forward pass through the linear layer\n",
    "output = linear_layer(x)\n",
    "\n",
    "# Print the input, weights, bias, and output\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nWeights:\")\n",
    "print(linear_layer.weight)\n",
    "print(\"\\nBias:\")\n",
    "print(linear_layer.bias)\n",
    "print(\"\\nOutput:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have multiple layers - as we often do - we need to make sure that the neurons match. The output of the first layer must match the input of the second layer, and so on. This is why we need to specify the number of input units for the first layer and the number of output units for the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Linear(4, 5)  # 4 input neurons to 5 hidden neurons\n",
    "l2 = nn.Linear(5, 1)  # 5 hidden neurons to 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In a fully connected layer, the input $x \\in \\mathbb R^{M \\times C_{in}}$ is a vector (or, rather a batch of vectors), where $M$ is the minibatch size and $C_{in}$ is the dimensionality of the input. \n",
    "We first matrix multiply the input $x$ by a weight matrix $W$.\n",
    "This weight matrix has dimensions $W \\in \\mathbb R^{C_{in} \\times C_{out}}$, where $C_{out}$ is the number of output units.\n",
    "We then add a bias for each output, which we do by adding $b \\in \\mathbb{R}^{C_{out}}$.\n",
    "The output $y \\in \\mathbb{R}^{M \\times C_{out}}$ of the fully connected layer then:\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "y = \\text{ReLU}(x W + b)\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "*Remember, the values of $W$ and $b$ are variables that we are trying to learn for our model*. These are the parameters of the model that we update during training.\n",
    "\n",
    "Below we have a visualization of what the matrix operation looks like (bias term and activation function omitted).\n",
    "\n",
    "<img src=\"../media/mnist_matmul.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Keep in mind that just stacking linear layers on top of each other doesn't make the network any more powerful than a single layer. To introduce non-linearity, we apply an activation function to the output of each layer. Recall what we learned about the **ReLU** activation function earlier, and let's add it to our budding neural network.\n",
    "\n",
    "Letâ€™s modify our previous example by applying a ReLU activation function to the output of the linear layer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a single linear layer\n",
    "linear_layer = nn.Linear(4, 5)  # 4 input neurons to 5 output neurons\n",
    "\n",
    "# Example input\n",
    "x = torch.randn(3, 4)  # A batch of 3 examples, each with 4 input features\n",
    "\n",
    "# Forward pass through the linear layer\n",
    "linear_output = linear_layer(x)\n",
    "\n",
    "# Apply ReLU activation function\n",
    "relu_output = F.relu(linear_output)\n",
    "\n",
    "# Print the results\n",
    "print(\"Linear Layer Output (Pre-Activation):\")\n",
    "print(linear_output)\n",
    "print(\"\\nOutput After ReLU Activation:\")\n",
    "print(relu_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>REVIEW</strong></div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p><b>Neurons</b>: Individual units in a neural network that process information. They receive input from the data and apply a transformation to it. The output of this transformation is then passed to the next layer of neurons.</p>\n",
    "<p><b>Linear layers</b>: Layers of neurons in a neural network that perform a linear transformation of the input. The output is computed by applying the linear transformation to the input tensor.</p>\n",
    "<p><b>Activation functions</b>: Functions that introduce non-linearity to the output of a neuron. They are applied to the output of each layer to make the network more powerful.</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understanding of fully connected layers and activation functions, let's see how we can use them to build a simple neural network in PyTorch. The following code is implemented **functionally**, meaning that we don't use a neat sequential container (a `nn.Module` class, as we will see later). The input will still be random, but hopefully you may be able to see how the layers are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a random flat input vector\n",
    "# Simulates input from a flattened 28x28 image (e.g., MNIST images) for a batch of 100 examples\n",
    "x_fc = torch.randn(100, 784)\n",
    "\n",
    "# Create weight matrix variable\n",
    "# Randomly initialize weights for a fully connected layer with 784 input features and 10 output features\n",
    "# The weights are scaled by 1/sqrt(784) for better initialization stability\n",
    "W = torch.randn(784, 10) / sqrt(784)\n",
    "W.requires_grad_()  # Enable gradient computation for optimization\n",
    "\n",
    "# Create bias variable\n",
    "# Initialize bias for the 10 output features to zero\n",
    "# Bias is also trainable, so gradients are required\n",
    "b = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "# Apply fully connected layer\n",
    "# Compute the pre-activation output (y_preact) as a linear transformation of the input\n",
    "# y_preact = x_fc @ W + b\n",
    "y_preact = torch.matmul(x_fc, W) + b\n",
    "\n",
    "# Apply ReLU activation function\n",
    "# ReLU introduces non-linearity, setting negative values in y_preact to zero\n",
    "y = F.relu(y_preact)\n",
    "\n",
    "# Print input/output shape\n",
    "# Input: 100 samples of 784 features (flattened 28x28 images)\n",
    "# Output: 100 samples of 10 features (e.g., 10 classes for classification)\n",
    "print(f\"Input shape: {x_fc.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is finally time to build a custom neural network. We will be using the MNIST digit classification dataset. Here's how our neural network architecture will look: <br>\n",
    "\n",
    "- **Input layer**: 784 neurons (28x28 pixels)\n",
    "- **Hidden layer 1**: 128 neurons\n",
    "- **ReLU activation**\n",
    "- **Hidden layer 2**: 64 neurons\n",
    "- **ReLU activation**\n",
    "- **Output layer**: 10 neurons (for each digit 0-9)\n",
    "\n",
    "We can build this NN with the components introduced before, but it may prove helpful to instead organize our model with a `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MNIST_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Required for nn.Module class\n",
    "\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the forward pass\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our `nn.Module` contains several operation chained together.\n",
    "The code for submodule initialization, which creates all the stateful parameters associated with each operation, is placed in the `__init__()` function, where it is run once during object instantiation.\n",
    "Meanwhile, the code describing the forward pass, which is used every time the model is run, is placed in the `forward()` method.\n",
    "Printing an instantiated model shows the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_model = MNIST_NN()\n",
    "print(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need a **training loop** to train the model. This loop will:\n",
    "\n",
    "1. **Forward pass**: Compute the predicted output by passing inputs to the model\n",
    "2. **Loss computation**: Compute the loss using the predicted output and the actual output\n",
    "3. **Backward pass**: Compute the gradients of the loss with respect to the model parameters\n",
    "4. **Update the weights**: Update the model parameters using the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the data. The PyTorch library contains several datasets, including MNIST. We can use the `torchvision` module to download and load this dataset. The MNIST dataset contains 28x28 pixel images of handwritten digits (0-9) and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `DataLoader` class to create an iterable object that will return batches of images and labels during training. Notice that we define a batch size of 100, which means that each batch will contain 100 images and labels.\n",
    "We also shuffle the training data to ensure that the model does not learn the order of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "train_loader.dataset, test_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a series of training parameters. Specifically, we need the following\n",
    "\n",
    "- **Number of epochs**: The number of times the model will iterate over the entire training dataset\n",
    "- **Learning rate**: A hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient\n",
    "- **Loss function**: The function used to compute the loss between the predicted and actual labels. Often called the objective function or criterion.\n",
    "- **Optimizer**: The algorithm used to update the weights of the network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criterion / Loss Functions\n",
    "\n",
    "Our loss function determines how well the modelâ€™s predictions align with the actual labels. For classification tasks, one of the most commonly used loss functions is **Cross-Entropy Loss**, implemented in PyTorch as `nn.CrossEntropyLoss()`.\n",
    "\n",
    "Cross-Entropy Loss measures the difference between two probability distributions: the predicted probability distribution (output of the model) and the true distribution (represented by the labels). Itâ€™s particularly suited for multi-class classification problems. The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$: Number of samples in the batch.\n",
    "- $C$: Number of classes.\n",
    "- $y_{ij}$: Ground truth label for class $j$ of sample $i$ (usually a one-hot encoded vector).\n",
    "- $\\hat{y}_{ij}$: Model's predicted probability for class $j$ of sample $i$.\n",
    "\n",
    "In PyTorch, `nn.CrossEntropyLoss()` simplifies this process:\n",
    "- **Input**: Raw logits (unnormalized scores) from the model.\n",
    "- **Output**: A scalar value representing the average loss across the batch.\n",
    "\n",
    "Predictions that are far from the true label (low probability for the correct class) result in a larger loss. By working on predicted probabilities, the loss metric ensures that the model outputs meaningful confidence scores for each class.\n",
    "\n",
    "---\n",
    "\n",
    "Some alternatives to Cross-Entropy Loss:\n",
    "\n",
    "1. **Negative Log-Likelihood Loss (NLLLoss)**: Often used in conjunction with softmax probabilities as input. It is similar to Cross-Entropy Loss but requires explicit probability distributions as input. Example in PyTorch: `nn.NLLLoss()`.\n",
    "\n",
    "2. **Mean Squared Error Loss (MSELoss)**: Computes the squared difference between predicted and true labels. It is suitable for regression tasks but not ideal for classification due to lack of probabilistic interpretation. Example in PyTorch: `nn.MSELoss()`.\n",
    "\n",
    "4. **Binary Cross-Entropy Loss (BCELoss)**: Used for binary classification tasks. BCELoss computes the cross-entropy between the predicted probabilities and the true binary labels. Example in PyTorch: `nn.BCELoss()`.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = torch.optim.SGD(first_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer\n",
    "\n",
    "To train our model, we need an optimizer that updates the modelâ€™s parameters to minimize the loss function. In this notebook, weâ€™ll use **Stochastic Gradient Descent (SGD)**, one of the most fundamental optimization algorithms. SGD updates the model parameters using the gradient of the loss function with respect to each parameter. The update rule for each parameter is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta L\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta$: Model parameter being updated (e.g., weights or biases),\n",
    "- $\\eta$: Learning rate, which controls the step size of the update,\n",
    "- $\\nabla_\\theta L$: Gradient of the loss $L$ with respect to $\\theta$.\n",
    "\n",
    "Instead of computing the gradient over the entire dataset, SGD updates parameters using a small batch, which is faster for large datasets. Itâ€™s easy to implement and serves as the foundation for more advanced optimizers.\n",
    "\n",
    "---\n",
    "\n",
    "Some alternatives to SGD:\n",
    "\n",
    "1. **Adam**: A very popular adaptive learning rate optimization algorithm thatâ€™s well-suited for deep learning. It combines the best properties of Adagrad and RMSprop to provide an efficient optimization method. Usage in PyTorch: `torch.optim.Adam()`.\n",
    "\n",
    "2. **SGD with Momentum**: An extension of basic SGD that includes a momentum term to accelerate convergence in the relevant direction and dampen oscillations. Usage in PyTorch: `torch.optim.SGD(momentum=0.9)`\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "The **learning rate** ($\\eta$ or `lr` in the code cell above) is a hyperparameter in training neural networks that controls how much the model's parameters are updated during each step of optimization. In other words, the learning rate determines the size of the steps the optimizer takes toward minimizing the loss function.\n",
    "\n",
    "<img src=\"../media/learning_rate.png\" width=\"500\"/>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>The right learning rate</strong></div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p><b>Too High</b>: A large learning rate can cause the model to overshoot the optimal parameters, resulting in divergence or instability during training.<p/>\n",
    "<p><b>Too Low</b>: A small learning rate leads to slow convergence, increasing the time required for training.<p/>\n",
    "<p><b>Just Right</b>: A properly tuned learning rate balances fast convergence with stable updates, enabling the model to reach optimal performance efficiently.<p/>\n",
    "\n",
    "<p>Choosing the right learning rate often requires experimentation. A common approach is to start with a small value (e.g., 0.01) and adjust based on training behavior. If the loss decreases too slowly, increase the learning rate. If the loss fluctuates or diverges, decrease the learning rate.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use GPU if available\n",
    "\n",
    "The function below (`get_device`) will use torch to find out if a GPU is available. If it is, the function will return the string `cuda`, which we can use to move our model and data to the GPU. If not, it will return `cpu`.\n",
    "\n",
    "When we want to use the device, we need to ping it with `model.to(device)` and `data.to(device)`, as you will see below in our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._prims_common import DeviceLikeType\n",
    "\n",
    "\n",
    "def get_device() -> DeviceLikeType:\n",
    "    \"\"\"Function to determine whether to run the training on GPU or CPU.\"\"\"\n",
    "\n",
    "    # Device will determine whether to run the training on GPU or CPU.\n",
    "    if torch.backends.mps.is_available():  # GPU on MacOS\n",
    "        device = \"mps\"\n",
    "    elif torch.cuda.is_available():  # GPU on Linux/Windows\n",
    "        device = \"cuda\"\n",
    "    else:  # default to CPU if no GPU is available\n",
    "        device = \"cpu\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    print(f\"Running pytorch version ({torch.__version__}) with backend = {device}\")\n",
    "\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GWe also need to send the model to the device before we start training. This is done by calling `model.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_NN().to(DEVICE)  # move the model to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting our training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # our loss function / objective function / criterion\n",
    "\n",
    "learning_rate = 0.001 # how \"fast\" we want to learn\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # optimizer\n",
    "\n",
    "num_epochs = 20 # how many epochs we want to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_val = datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_train_val[0]\n",
    "\n",
    "print(\"# first two rows of the first channel of the image:\")\n",
    "print(image[0][:2], \"\\n\")\n",
    "print(f\"{image.shape=}\")\n",
    "print(f\"The label is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you well know, we always want to keep a validation set separate from the training set. This is because the model can overfit to the training data, meaning it learns to perform well on the training data but fails to generalize to new, unseen data. The validation set helps us monitor the model's performance on unseen data and detect overfitting.\n",
    "<br><br>Let's leave 20% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# split the train_val data into train and validation\n",
    "val_ratio = 0.2 # 20% of the data will be used for validation\n",
    "\n",
    "total_size = len(mnist_train_val)\n",
    "val_size = int(total_size * val_ratio)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "# Split the dataset into training and validation datasets\n",
    "train_dataset, val_dataset = random_split(mnist_train_val, [train_size, val_size])\n",
    "\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "f\"{len(train_loader.dataset)=}, {len(val_loader.dataset)=}, {len(test_loader.dataset)=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "Now that we understand the components of the training loop, we can train the model. The training process involves iterating over the entire dataset multiple times (epochs) and updating the model parameters to minimize the loss function. We will also evaluate the model on a separate validation dataset to monitor its performance.\n",
    "\n",
    "Specifically, the training loop will, for each epoch:\n",
    "1. **Forward pass**: Compute the predicted output by passing inputs to the model\n",
    "2. **Loss computation**: Compute the loss using the predicted output and the actual output\n",
    "3. **Backward pass**: Compute the gradients of the loss with respect to the model parameters\n",
    "4. **Update the weights**: Update the model parameters using the gradients\n",
    "5. **Validation**: Compute the accuracy of the model on the validation dataset\n",
    "6. **log history**: Store the loss and accuracy for the epoch\n",
    "\n",
    "This may sound like a lot but each step is thoroughly documented in the code below. Read through it and feel free to ask any questions in class / the lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history trackers\n",
    "# We will use these to visualize the training progress so we can analyze the model's performance\n",
    "# and decide if we need to adjust hyperparameters (e.g., learning rate, number of epochs)\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(num_epochs):  # Remember we set num_epochs to 5\n",
    "\n",
    "    #################\n",
    "    # TRAINING PHASE\n",
    "    #################\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    train_loss = 0.0  # Variable to accumulate the training loss\n",
    "    correct = 0  # Variable to count correctly predicted samples\n",
    "    total = 0  # Total number of samples\n",
    "\n",
    "    for (images, labels) in train_loader:  # Loop through batches\n",
    "\n",
    "        # Move data to the selected device (CPU or GPU)\n",
    "        # remember how we set the \"device\" earlier?\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Flatten the images into 1D vectors (if necessary for fully connected layers)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        \n",
    "        ###################\n",
    "        # FORWARD PASS\n",
    "        ###################\n",
    "\n",
    "        # Compute model outputs and loss\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate loss using the cross-entropy loss function\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        ###################\n",
    "        # BACKWARD PASS: COMPUTE GRADIENTS\n",
    "        ###################\n",
    "\n",
    "        # Clear previous gradients. This is necessary because\n",
    "        # gradients are accumulated by default (useful for RNNs, etc.)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute new gradients. Gradient of the loss w.r.t. model parameters\n",
    "        # think of it like the slope of the loss function: how should we adjust the weights\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters based on the computed gradients\n",
    "        # i.e. take a step in the \"right\" direction, as determined by the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy metrics\n",
    "        train_loss += loss.item() * images.size(0)  # Loss multiplied by batch size\n",
    "\n",
    "        # Get the predicted class (the class with the highest probability)\n",
    "        # by selecting the class with the highest score from the output layer\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "\n",
    "        total += labels.size(0)  # Update total number of samples\n",
    "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Calculate average training loss and accuracy for the current epoch\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    ###################\n",
    "    # VALIDATION PHASE\n",
    "    ###################\n",
    "\n",
    "    # In the validation phase, we do almost the exact same thing as \n",
    "    # we do for training, except we do not compute gradients nor\n",
    "    # update the model parameters. We only evaluate the model's performance.\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0  # Accumulate validation loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed during validation\n",
    "        for images, labels in val_loader:\n",
    "\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # Save metrics to history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_accuracy)\n",
    "    history[\"val_acc\"].append(val_accuracy)\n",
    "\n",
    "    # Print progress for the current epoch\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}]: \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}% | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting loss and accuracy\n",
    "\n",
    "We can visualize the training process by plotting the loss and accuracy over time. This helps us understand how the model is learning and whether it is overfitting to the training data. \n",
    "\n",
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>Overfitting and Underfitting in Training/Validation Plots</strong></div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p><b>Overfitting</b>: The training loss decreases significantly and training accuracy increases, but validation loss stops improving (or increases) while validation accuracy plateaus or decreases. This indicates the model is learning patterns specific to the training data and failing to generalize to unseen data.<p/>\n",
    "\n",
    "<p><b>Underfitting</b>: Both training and validation loss remain high, and neither training nor validation accuracy improves significantly. This suggests the model is too simple or has not been trained for enough epochs to capture meaningful patterns in the data.<p/>\n",
    "\n",
    "<p><b>Good Generalization</b>: Training and validation loss decrease steadily, and validation accuracy improves alongside training accuracy. The curves remain closely aligned, indicating the model is capturing meaningful features and generalizing well to unseen data.<p/>\n",
    "\n",
    "<p>To address overfitting, you can try techniques like regularization (e.g., dropout, weight decay) or early stopping. For underfitting, consider increasing model complexity, training for more epochs, or adjusting hyperparameters like the learning rate.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "We can use the `matplotlib` library to create these plots from our training history. The code below plots the training and validation loss and accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation losses and accuracies from the history dictionary.\n",
    "\n",
    "    Args:\n",
    "        history (dict): A dictionary containing training history with keys:\n",
    "                        - 'train_loss': List of training losses.\n",
    "                        - 'val_loss': List of validation losses.\n",
    "                        - 'train_acc': List of training accuracies.\n",
    "                        - 'val_acc': List of validation accuracies.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Plot Losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)  # Create a subplot for losses\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracies\n",
    "    plt.subplot(1, 2, 2)  # Create a subplot for accuracies\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\", marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing our plots \n",
    "\n",
    "These plots show the training and validation loss (left) and accuracy (right) over the course of training epochs.\n",
    "\n",
    "Both training and validation loss steadily decrease and remain closely aligned throughout the epochs. This indicates that the model is learning effectively without signs of overfitting. We also see that training and validation accuracy show consistent improvement over epochs, eventually plateauing as they converge. This suggests stable training and good generalization to unseen data.\n",
    "\n",
    "Notice how the trend suggest that we could train for more epochs to improve the model's performance. Since this is a demo example, we will let it rest for now but feel free to experiment with the number of epochs and other hyperparameters to see how they affect the model's performance.\n",
    "\n",
    "##### How to handle overfitting\n",
    "While we don't overfit the training data in this example, it's a common problem in deep learning. Overfitting occurs when the model learns to perform well on the training data but fails to generalize to new, unseen data. This can happen when the model is too complex or when the training data is limited. Essentially, it becomes economical for the model to memorize the training data rather than learn general patterns.\n",
    "\n",
    "Here are some common strategies to prevent overfitting:\n",
    "\n",
    "* **Regularization**: Techniques like L1 and L2 regularization add a penalty term to the loss function, discouraging large weights and reducing model complexity. In PyTorch, we can add regularization to the optimizer using the `weight_decay` parameter, like so\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "```\n",
    "\n",
    "* **Dropout layers**: Dropout layers randomly set a fraction of input units to zero during training, preventing the model from relying too heavily on specific neurons. We can add dropout layers to our model using `nn.Dropout`.\n",
    "\n",
    "```python\n",
    "# other layers\n",
    "nn.Dropout(0.5)\n",
    "# other layers\n",
    "```\n",
    "\n",
    "* **Early stopping**: Monitor the model's performance on a validation set and stop training when performance starts to degrade. This prevents the model from overfitting to the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "##### Evaluation\n",
    "\n",
    "Now that we have trained the model, we can evaluate its performance on the test dataset. This dataset contains images that the model has never seen before, allowing us to assess how well the model generalizes to new, unseen data.\n",
    "\n",
    "It is also common practice to do a final evaluation on the training data and validation data - as a sanity check - to ensure that the model is not overfitting to the training data.\n",
    "\n",
    "We can use the `classification_report` function from the `sklearn.metrics` module to generate a detailed report of the model's performance on the test dataset. This report includes metrics such as precision, recall, and F1 score for each class, as well as the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_labels = []  # Collect all true labels\n",
    "all_predictions = []  # Collect all predicted labels\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Flatten the images\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Update accuracy metrics\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate metrics\n",
    "avg_loss = total_loss / len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Generate and print classification report\n",
    "class_names = test_loader.dataset.classes\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save/load the model\n",
    "\n",
    "Once we have trained the model, we can save it to disk using the `torch.save()` function. This allows us to load the model later for inference or further training without having to retrain it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"mnist_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = MNIST_NN()  # Initialize the model architecture\n",
    "model.load_state_dict(torch.load(\"mnist_model.pth\", weights_only=True))  # Load weights into the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load a single image (e.g., from test_loader)\n",
    "image, label = next(iter(test_loader))  # Get a batch from DataLoader\n",
    "single_image = image[0].unsqueeze(0)  \n",
    "single_label = label[0].item()\n",
    "\n",
    "\n",
    "# Flatten the image if your model expects a flat vector (e.g., for a fully connected layer)\n",
    "single_image = single_image.view(single_image.size(0), -1)  # Reshape to [1, 784] if necessary\n",
    "\n",
    "# Make a prediction\n",
    "output = model(single_image)  # Forward pass\n",
    "_, predicted = torch.max(output, 1)  # Get the predicted class index\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted class: {predicted.item()}\")\n",
    "print(f\"True class: {label[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now built and evaluated a neural network model for image classification using the MNIST dataset. You have learned how to define a neural network architecture, load and preprocess data, train the model, and evaluate its performance. This is a significant milestone in your journey to mastering deep learning ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03c821f7cc304647bd7dd129ece95022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_a9254a18982444839701b4f3c20b1113",
       "max": 600,
       "style": "IPY_MODEL_feba469114dd47aaa7a13bc21b134099",
       "value": 600
      }
     },
     "06865ecae49c4d328421c474fea799a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "09e99596521e40948b044aea1dd49ceb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0a271956e4984a0a9ba69c1f66809bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6ca5eb76ff8441cc996cdbc8bcfb5ab3",
       "style": "IPY_MODEL_eb483872c21d43d4a42b34091b4eb8c1",
       "value": " 600/600 [05:31&lt;00:00,  1.81it/s]"
      }
     },
     "0edf80c192134246a102f49c1ac4a77e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "13e3067366c34bfdbd817690ebeb5685": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "15ada75a5a524e0d80956489b169679b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1d856c2263ec4b1f97f46727b8d85f12": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e7d4fd24b054b3daa60ae7dd2726ec5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6933964202a74264b0ed487f706297a3",
        "IPY_MODEL_b8590f98675a453ba38b85163a285eb4"
       ],
       "layout": "IPY_MODEL_ba35f1bbfe5549f394fcb2a1b544584b"
      }
     },
     "234c9d9fbbaa4aaa84dfd3598983e96a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26e0d143239647a2832a3d656f56fd12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2779485b2912403f80088f678d5a01c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "28a7ac6f810e4b4a94503dbda7ebf78c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "2d0476b37d43454585abb625c1c50181": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2da8ddac80d74ca3ab5fd5efcefbd0b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c047c4ef8c3c4f1fa8508267806dc5c2",
        "IPY_MODEL_35e061d7f6aa4817a081a3ce2416bc7d"
       ],
       "layout": "IPY_MODEL_62ef7f1b4c0f4a008c366a21cec33c17"
      }
     },
     "2e67a47c98ae4b19a913886efd194566": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_bb81062e385c414abc8c3f8a70716327",
       "max": 600,
       "style": "IPY_MODEL_731297e272154e4ab71e7fd6a6859309",
       "value": 600
      }
     },
     "3152dae7252a4ee3958217f48a9bb0c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "322ecba10f114730888b6b15a8694fea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "34484a12bd9f494a9bb6c6e479524246": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "35e061d7f6aa4817a081a3ce2416bc7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5dfd065b0af94b72b0a797c1659d58e4",
       "style": "IPY_MODEL_13e3067366c34bfdbd817690ebeb5685",
       "value": " 600/600 [04:52&lt;00:00,  2.05it/s]"
      }
     },
     "371bf011ef8c488fac1fae1c63dc7dae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6f06679d39f84b75a08a05e478e0bc89",
       "style": "IPY_MODEL_cadc3c445e7444f5af275f20c674019b",
       "value": " 100/100 [00:18&lt;00:00,  5.43it/s]"
      }
     },
     "40143d1b96fa4cbdb4ea4f915ac0e7c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_34484a12bd9f494a9bb6c6e479524246",
       "style": "IPY_MODEL_d274246f37a643ba879a390b788b3c5b",
       "value": " 100/100 [00:18&lt;00:00,  5.32it/s]"
      }
     },
     "456c3368617d4acd9b81809d2643f847": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "526b7dd6def34ac68dfd667d9ecc7373": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "55697ad8b13f40eb900098274954edff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_15ada75a5a524e0d80956489b169679b",
       "style": "IPY_MODEL_783576334fac49bbb171283f192725f2",
       "value": " 3/3 [28:40&lt;00:00, 573.52s/it]"
      }
     },
     "5c35d2a0736845aca0c743382fb21435": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_968de7cc30cc49219edb47c77ac19835",
       "style": "IPY_MODEL_26e0d143239647a2832a3d656f56fd12",
       "value": " 600/600 [06:08&lt;00:00,  1.63it/s]"
      }
     },
     "5c778265dd2e4f55990114a410be07e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_1d856c2263ec4b1f97f46727b8d85f12",
       "style": "IPY_MODEL_3152dae7252a4ee3958217f48a9bb0c7",
       "value": 100
      }
     },
     "5dfd065b0af94b72b0a797c1659d58e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5dfea9beebf347babc96499b6ef6e55b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5fa779faf29448bda9ecb19f04db2363": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "62ef7f1b4c0f4a008c366a21cec33c17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68181c92926e4b209ebddc72e0a18a90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6933964202a74264b0ed487f706297a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "  0%",
       "layout": "IPY_MODEL_88946e6e9e3d45c2a58c6712f7b78fef",
       "max": 3,
       "style": "IPY_MODEL_28a7ac6f810e4b4a94503dbda7ebf78c"
      }
     },
     "6ca5eb76ff8441cc996cdbc8bcfb5ab3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ebc572251db40378e419fa603857e39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6f06679d39f84b75a08a05e478e0bc89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6faf5a48daed4ceb8b42b918f5890080": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "70bfccb7e1144fc3a04cd0ca72ae63eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f67d8b42eb0b4156929a609efc6e9708",
        "IPY_MODEL_55697ad8b13f40eb900098274954edff"
       ],
       "layout": "IPY_MODEL_6faf5a48daed4ceb8b42b918f5890080"
      }
     },
     "731297e272154e4ab71e7fd6a6859309": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "783576334fac49bbb171283f192725f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "78f433d8b75e4a08a36465e145079cf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7adead9aa0e54151b5bae6180e87eaf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "7cd916a53b1a42caa0d8fab7855e43bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5c778265dd2e4f55990114a410be07e7",
        "IPY_MODEL_371bf011ef8c488fac1fae1c63dc7dae"
       ],
       "layout": "IPY_MODEL_d1536ee4c2b94f5d819c52777db82fd1"
      }
     },
     "7d6bdfc513274cbc997b980fd0bf6d4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_b4d97b0af743462fa7ab900547f28cec",
       "max": 3,
       "style": "IPY_MODEL_b2374c1841914c56a1c0a0ce0fdd8f52",
       "value": 3
      }
     },
     "7e775f18ba894cad88b398c06546947d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "7f1e0c9662724b0a94f6412f23d4f48b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_526b7dd6def34ac68dfd667d9ecc7373",
       "style": "IPY_MODEL_7adead9aa0e54151b5bae6180e87eaf2",
       "value": 100
      }
     },
     "806fdae35ae54e4398bc198978f1143a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_06865ecae49c4d328421c474fea799a0",
       "style": "IPY_MODEL_6ebc572251db40378e419fa603857e39",
       "value": " 600/600 [17:51&lt;00:00,  1.79s/it]"
      }
     },
     "8334eba58bc34ceeb9c15edb42401d3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2e67a47c98ae4b19a913886efd194566",
        "IPY_MODEL_9301e0239a174dfc9b221ec2a02af44f"
       ],
       "layout": "IPY_MODEL_5dfea9beebf347babc96499b6ef6e55b"
      }
     },
     "88946e6e9e3d45c2a58c6712f7b78fef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "88e7221416214664b30773fed9a88614": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "8cecc0e1039c4eee90dfa2db1afab3c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "90722b21a44e4541a5333511f9e1e3c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9301e0239a174dfc9b221ec2a02af44f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d9151e052ab545159f94594f8d155fd7",
       "style": "IPY_MODEL_e45c675d8d044d7c9550a20e40b0539e",
       "value": " 600/600 [05:17&lt;00:00,  1.89it/s]"
      }
     },
     "968de7cc30cc49219edb47c77ac19835": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "96c234a72edc42ca80e536e3a54c2acc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a229f6b4ca20475ea3a5745d18d7131c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_322ecba10f114730888b6b15a8694fea",
       "style": "IPY_MODEL_a8a4eb83499a453787b42fc46b93a0ba",
       "value": " 223/600 [01:52&lt;03:02,  2.06it/s]"
      }
     },
     "a83b638605be4a9ba18c4529729de544": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a8a4eb83499a453787b42fc46b93a0ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a9254a18982444839701b4f3c20b1113": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "abc221956ffb4578aec132dddb6ef921": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "acb0ac60dd0643c6b1ae204792f442d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "acc0927c8ae7415f9df9b49feeadfded": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": " 37%",
       "layout": "IPY_MODEL_2d0476b37d43454585abb625c1c50181",
       "max": 600,
       "style": "IPY_MODEL_88e7221416214664b30773fed9a88614",
       "value": 223
      }
     },
     "aff6325150904f36b4dae74956e87bff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_03c821f7cc304647bd7dd129ece95022",
        "IPY_MODEL_806fdae35ae54e4398bc198978f1143a"
       ],
       "layout": "IPY_MODEL_78f433d8b75e4a08a36465e145079cf1"
      }
     },
     "b18a79c6165442b7ac686c25eb915836": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b1ce822ea1f44e69b04dfabf5d6b176b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7f1e0c9662724b0a94f6412f23d4f48b",
        "IPY_MODEL_40143d1b96fa4cbdb4ea4f915ac0e7c7"
       ],
       "layout": "IPY_MODEL_8cecc0e1039c4eee90dfa2db1afab3c1"
      }
     },
     "b2374c1841914c56a1c0a0ce0fdd8f52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "b4d97b0af743462fa7ab900547f28cec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b55d6f340ec0428eac1c4ba7262fda73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7d6bdfc513274cbc997b980fd0bf6d4b",
        "IPY_MODEL_c4a75cbcd5784be48fdfb09dbacbd3a3"
       ],
       "layout": "IPY_MODEL_acb0ac60dd0643c6b1ae204792f442d4"
      }
     },
     "b7e465f909304b8da8f19b67743c925f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b8590f98675a453ba38b85163a285eb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_decefda30d7f4c59a15bfd79f082045b",
       "style": "IPY_MODEL_cf367f49b29745fe88c33e2b2f46f8e4",
       "value": " 0/3 [00:00&lt;?, ?it/s]"
      }
     },
     "b9c879ca534d4691a94bd015b53d7967": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba35f1bbfe5549f394fcb2a1b544584b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bb81062e385c414abc8c3f8a70716327": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c047c4ef8c3c4f1fa8508267806dc5c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_456c3368617d4acd9b81809d2643f847",
       "max": 600,
       "style": "IPY_MODEL_dccbf0d594854417975e06c2251f4f92",
       "value": 600
      }
     },
     "c18eea10e4ff4974b07ff2253f5f779d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_09e99596521e40948b044aea1dd49ceb",
       "max": 600,
       "style": "IPY_MODEL_d6c628ea9a2347579e8b04a34dae7e83",
       "value": 600
      }
     },
     "c4a75cbcd5784be48fdfb09dbacbd3a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_90722b21a44e4541a5333511f9e1e3c8",
       "style": "IPY_MODEL_b18a79c6165442b7ac686c25eb915836",
       "value": " 3/3 [16:33&lt;00:00, 331.17s/it]"
      }
     },
     "c6a40c98b57d4d99ad89de894e76aafb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_a83b638605be4a9ba18c4529729de544",
       "max": 600,
       "style": "IPY_MODEL_abc221956ffb4578aec132dddb6ef921",
       "value": 600
      }
     },
     "cadc3c445e7444f5af275f20c674019b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cf367f49b29745fe88c33e2b2f46f8e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d0160fcf3a1c4409aa417bc54dcd2029": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f9ae08e19e6540a2ab502af5ff456067",
        "IPY_MODEL_fe492931e45e4f9995c1302483fde1ba"
       ],
       "layout": "IPY_MODEL_b9c879ca534d4691a94bd015b53d7967"
      }
     },
     "d1536ee4c2b94f5d819c52777db82fd1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d274246f37a643ba879a390b788b3c5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d34d970aa5e84bd8b30f3c0af6851044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_acc0927c8ae7415f9df9b49feeadfded",
        "IPY_MODEL_a229f6b4ca20475ea3a5745d18d7131c"
       ],
       "layout": "IPY_MODEL_68181c92926e4b209ebddc72e0a18a90"
      }
     },
     "d6c628ea9a2347579e8b04a34dae7e83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "d9151e052ab545159f94594f8d155fd7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dccbf0d594854417975e06c2251f4f92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "de12b4d028d240d88666934d195132db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "decefda30d7f4c59a15bfd79f082045b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e45c675d8d044d7c9550a20e40b0539e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e59571b38ab04469b7e1af29747dd20d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c6a40c98b57d4d99ad89de894e76aafb",
        "IPY_MODEL_0a271956e4984a0a9ba69c1f66809bfb"
       ],
       "layout": "IPY_MODEL_0edf80c192134246a102f49c1ac4a77e"
      }
     },
     "eb483872c21d43d4a42b34091b4eb8c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f67d8b42eb0b4156929a609efc6e9708": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_de12b4d028d240d88666934d195132db",
       "max": 3,
       "style": "IPY_MODEL_2779485b2912403f80088f678d5a01c0",
       "value": 3
      }
     },
     "f9ae08e19e6540a2ab502af5ff456067": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_234c9d9fbbaa4aaa84dfd3598983e96a",
       "max": 600,
       "style": "IPY_MODEL_7e775f18ba894cad88b398c06546947d",
       "value": 600
      }
     },
     "fce41ea4b3b74dc2ab55a27f6578cbbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c18eea10e4ff4974b07ff2253f5f779d",
        "IPY_MODEL_5c35d2a0736845aca0c743382fb21435"
       ],
       "layout": "IPY_MODEL_b7e465f909304b8da8f19b67743c925f"
      }
     },
     "fe492931e45e4f9995c1302483fde1ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_96c234a72edc42ca80e536e3a54c2acc",
       "style": "IPY_MODEL_5fa779faf29448bda9ecb19f04db2363",
       "value": " 600/600 [05:32&lt;00:00,  1.81it/s]"
      }
     },
     "feba469114dd47aaa7a13bc21b134099": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
