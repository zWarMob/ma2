{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bit of a hack in case your IDE wants to run the notebook from `/guides/` and not the project root folder `/ma2`. We need the working directory to be `/ma2` for local imports to work. Why? Well, the 'src' folder is in the project root, and we need to import the modules from there. Without this hack, the imports will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/nicolai/Desktop/cbs/aiml25/ma2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the working directory is set to the \"ma1\" folder.\n",
    "while Path.cwd().name != \"ma2\" and \"ma2\" in str(Path.cwd()): \n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "- Bag of Words (BoW) is a fundamental text representation technique in Natural Language Processing (NLP) that transforms text into numerical features by counting word occurrences while disregarding grammar and word order. This simplistic yet powerful approach forms the foundation for many classical NLP models and serves as a stepping stone to more sophisticated text analysis methods.\n",
    "\n",
    "- The concept of BoW was formalized in early information retrieval systems and gained prominence with the Vector Space Model ([Salton et al., 1975](https://dl.acm.org/doi/10.1145/361219.361220)), which represents documents as vectors in a high-dimensional space where each dimension corresponds to a distinct term in the vocabulary. This representation enables mathematical operations on text data, facilitating document comparison through similarity metrics like cosine similarity.\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF) ([Sparck Jones, 1972](https://www.doi.org/10.1108/eb026562)) enhanced the basic BoW model by *weighting terms based on their frequency in a document relative to their occurrence across a corpus*. This addressed the limitation of treating all terms equally and has become a standard weighting scheme in information retrieval and text classification tasks.\n",
    "\n",
    "- The BoW approach is still widely used in industry despite its simplicity. BoW-based models demonstrate surprising effectiveness for tasks like sentiment analysis, spam detection, and document categorization. Since the beginning of the deep learning era, BoW is often used as a baseline for comparing the performance of more complex models. Generally speaking, simpler is bettet!\n",
    "\n",
    "- BoW has notable limitations: it loses contextual information, struggles with synonyms and polysemy, and creates sparse high-dimensional vectors. Modern NLP has largely transitioned to distributed word representations like **Word2Vec** ([Mikolov et al., 2013](https://arxiv.org/abs/1301.3781)) and contextualized embeddings from transformer-based models, which capture semantic relationships and contextual nuances that BoW approaches fundamentally cannot represent.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, we will** \n",
    "\n",
    "1. Explore how to build a BoW model from scratch using Python, as a motivating example. This way we can see the simple steps involved in building BoW models\n",
    "\n",
    "2. **Build a BoW model with scikit-learn**: This is the common way to build BoW models. This section is meant to provide inspiration for the BoW section of the assignment. Here we work with a subset of the [AG News](https://huggingface.co/datasets/fancyzhx/ag_news) dataset, which you will also use for the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div style=\"padding-left: 10px; padding-right: 10px; padding-top: 10px; padding-bottom: 30px, align: justify\">\n",
    "<p align=\"center\">\n",
    "<img src=\"../media/bow_slp1.png\" alt=\"Transformer Architecture\" width=\"800\"/><br>\n",
    "Image from Speech and Language Processing. Daniel Jurafsky & James H. Martin.<br> Copyright © 2024. All\n",
    "rights reserved. Draft of January 12, 2025.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# 1. An introduction to bag-of-words models\n",
    "\n",
    "* A bag-of-words model is a type of vector representation for text data that describes the occurrence of words within a document. We transform each document into a vector of word counts or frequencies. This vectorization enables standard machine learning methods.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "Any text (or document) can be transformed into an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the figure, instead of representing the word order in all the phrases like “<i>I love this movie</i>” and “<i>I would recommend it</i>”, we simply note that the word I occurred 5 times in the entire excerpt, the word it 6 times, the words love, recommend, and movie once, and so on. In other words, no pun intended, the position of the words is ignored (the bag-of-words assumption) and we make use of the *frequency* of each word.\n",
    "\n",
    "\n",
    "In this section, we will build a bag-of-words model from scratch. Let's assume we have the same text as in the image above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = (\n",
    "    \"I love this movie! It's sweet, but with satirical humor. \" +\n",
    "    \"The dialogue is great and the adventure scenes are fun... \" +\n",
    "    \"It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. \" +\n",
    "    \"I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!\"\n",
    ")\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the frequency of each word in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 3, 'love': 1, 'this': 1, 'movie!': 1, \"It's\": 1, 'sweet,': 1, 'but': 1, 'with': 1, 'satirical': 1, 'humor.': 1, 'The': 1, 'dialogue': 1, 'is': 1, 'great': 1, 'and': 3, 'the': 3, 'adventure': 1, 'scenes': 1, 'are': 1, 'fun...': 1, 'It': 1, 'manages': 1, 'to': 3, 'be': 1, 'whimsical': 1, 'romantic': 1, 'while': 1, 'laughing': 1, 'at': 1, 'conventions': 1, 'of': 1, 'fairy': 1, 'tale': 1, 'genre.': 1, 'would': 1, 'recommend': 1, 'it': 4, 'just': 1, 'about': 1, 'anyone.': 1, \"I've\": 1, 'seen': 2, 'several': 1, 'times,': 1, \"I'm\": 1, 'always': 1, 'happy': 1, 'see': 1, 'again': 1, 'whenever': 1, 'have': 1, 'a': 1, 'friend': 1, 'who': 1, \"hasn't\": 1, 'yet!': 1}\n"
     ]
    }
   ],
   "source": [
    "# split the document into words\n",
    "words = document.split() # split the document into words by whitespace\n",
    "\n",
    "# create a dictionary to store the frequency of each word\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will typically have more than one documents and we can build a BoW for any collection of texts. A set of documents is typically referred to as a corpus in NLP. Here we have a corpus of 8 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing (NLP) is a fascinating field of study, which involves the interaction between computers and humans using natural language.\",\n",
    "    \"The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\",\n",
    "    \"There are many challenges in NLP, such as dealing with the ambiguity and variability of natural language.\",\n",
    "    \"Techniques in NLP include tokenization, stemming, lemmatization, and part-of-speech tagging, among others.\",\n",
    "    \"Applications of NLP are vast and include machine translation, sentiment analysis, and speech recognition.\",\n",
    "    \"In recent years, deep learning has revolutionized NLP, leading to significant improvements in tasks like language modeling and text generation.\",\n",
    "    \"Despite these advancements, there are still many open problems in NLP, such as understanding context and handling low-resource languages.\",\n",
    "    \"Researchers in NLP are continually developing new methods and models to address these challenges and improve the performance of NLP systems.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditonal NLP - meaning the pre-transformer era - the preprocessing steps for text data were crucial. Transforming text data into a format that can be used by machine learning models is a non-trivial task. In the following, we will perform the following preprocessing steps:\n",
    "\n",
    "- Lowercase all text\n",
    "- Remove punctuation, and\n",
    "- Separate words using whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'movie', 'its', 'sweet', 'but', 'with', 'satirical', 'humor', 'the', 'dialogue', 'is', 'great', 'and', 'the', 'adventure', 'scenes', 'are', 'fun', 'it', 'manages', 'to', 'be', 'whimsical', 'and', 'romantic', 'while', 'laughing', 'at', 'the', 'conventions', 'of', 'the', 'fairy', 'tale', 'genre', 'i', 'would', 'recommend', 'it', 'to', 'just', 'about', 'anyone', 'ive', 'seen', 'it', 'several', 'times', 'and', 'im', 'always', 'happy', 'to', 'see', 'it', 'again', 'whenever', 'i', 'have', 'a', 'friend', 'who', 'hasnt', 'seen', 'it', 'yet']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "\n",
    "    # 1. Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation by building a new string without those characters\n",
    "    # Define punctuation characters explicitly (since we are not using any libraries)\n",
    "    punctuation_chars = \".,!?;:'\\\"()\"\n",
    "    text = ''.join(char for char in text if char not in punctuation_chars)\n",
    "\n",
    "    # 3. Tokenize by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Process the document and print results\n",
    "tokens = preprocess_text(document)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With preprocessing done, we can now count our words again. This time, we will have a much cleaner representation of our text data, since we have removed punctuation and converted all text to lowercase. for example, the word \"I\" and \"i\" will be counted as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preprocessed_documents)=8 | len(corpus)=8\n",
      "\n",
      "[['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study', 'which', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language'], ['the', 'goal', 'of', 'nlp', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful'], ['there', 'are', 'many', 'challenges', 'in', 'nlp', 'such', 'as', 'dealing', 'with', 'the', 'ambiguity', 'and', 'variability', 'of', 'natural', 'language'], ['techniques', 'in', 'nlp', 'include', 'tokenization', 'stemming', 'lemmatization', 'and', 'part-of-speech', 'tagging', 'among', 'others'], ['applications', 'of', 'nlp', 'are', 'vast', 'and', 'include', 'machine', 'translation', 'sentiment', 'analysis', 'and', 'speech', 'recognition'], ['in', 'recent', 'years', 'deep', 'learning', 'has', 'revolutionized', 'nlp', 'leading', 'to', 'significant', 'improvements', 'in', 'tasks', 'like', 'language', 'modeling', 'and', 'text', 'generation'], ['despite', 'these', 'advancements', 'there', 'are', 'still', 'many', 'open', 'problems', 'in', 'nlp', 'such', 'as', 'understanding', 'context', 'and', 'handling', 'low-resource', 'languages'], ['researchers', 'in', 'nlp', 'are', 'continually', 'developing', 'new', 'methods', 'and', 'models', 'to', 'address', 'these', 'challenges', 'and', 'improve', 'the', 'performance', 'of', 'nlp', 'systems']]\n",
      "len(word_freq)=142\n",
      "\n",
      "{'I': 3, 'love': 1, 'this': 1, 'movie!': 1, \"It's\": 1, 'sweet,': 1, 'but': 1, 'with': 2, 'satirical': 1, 'humor.': 1, 'The': 1, 'dialogue': 1, 'is': 4, 'great': 1, 'and': 14, 'the': 7, 'adventure': 1, 'scenes': 1, 'are': 5, 'fun...': 1, 'It': 1, 'manages': 1, 'to': 7, 'be': 1, 'whimsical': 1, 'romantic': 1, 'while': 1, 'laughing': 1, 'at': 1, 'conventions': 1, 'of': 6, 'fairy': 1, 'tale': 1, 'genre.': 1, 'would': 1, 'recommend': 1, 'it': 4, 'just': 1, 'about': 1, 'anyone.': 1, \"I've\": 1, 'seen': 2, 'several': 1, 'times,': 1, \"I'm\": 1, 'always': 1, 'happy': 1, 'see': 1, 'again': 1, 'whenever': 1, 'have': 1, 'a': 3, 'friend': 1, 'who': 1, \"hasn't\": 1, 'yet!': 1, 'natural': 3, 'language': 5, 'processing': 1, 'nlp': 9, 'fascinating': 1, 'field': 1, 'study': 1, 'which': 1, 'involves': 1, 'interaction': 1, 'between': 1, 'computers': 2, 'humans': 1, 'using': 1, 'goal': 1, 'enable': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'human': 1, 'in': 7, 'way': 1, 'that': 1, 'both': 1, 'meaningful': 1, 'useful': 1, 'there': 2, 'many': 2, 'challenges': 2, 'such': 2, 'as': 2, 'dealing': 1, 'ambiguity': 1, 'variability': 1, 'techniques': 1, 'include': 2, 'tokenization': 1, 'stemming': 1, 'lemmatization': 1, 'part-of-speech': 1, 'tagging': 1, 'among': 1, 'others': 1, 'applications': 1, 'vast': 1, 'machine': 1, 'translation': 1, 'sentiment': 1, 'analysis': 1, 'speech': 1, 'recognition': 1, 'recent': 1, 'years': 1, 'deep': 1, 'learning': 1, 'has': 1, 'revolutionized': 1, 'leading': 1, 'significant': 1, 'improvements': 1, 'tasks': 1, 'like': 1, 'modeling': 1, 'text': 1, 'generation': 1, 'despite': 1, 'these': 2, 'advancements': 1, 'still': 1, 'open': 1, 'problems': 1, 'understanding': 1, 'context': 1, 'handling': 1, 'low-resource': 1, 'languages': 1, 'researchers': 1, 'continually': 1, 'developing': 1, 'new': 1, 'methods': 1, 'models': 1, 'address': 1, 'improve': 1, 'performance': 1, 'systems': 1}\n"
     ]
    }
   ],
   "source": [
    "preprocessed_documents = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "print(f\"{len(preprocessed_documents)=} | {len(corpus)=}\\n\")\n",
    "print(preprocessed_documents)\n",
    "\n",
    "# Create word_freq = {}\n",
    "for tokens in preprocessed_documents:\n",
    "    for word in tokens:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "print(f\"{len(word_freq)=}\\n\")\n",
    "print(word_freq) # a dictionary to store the frequency of each word in the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we go from a dictionary of word counts to a vector representation? Think of the dictionary we have as a feature space. For each document, we can represent it as a vector where each element in the vector corresponds to the count of a word in the dictionary. This is the bag-of-words model.\n",
    "\n",
    "To make this more concrete, we need to build a document term matrix: a matrix where each row corresponds to a document and each column corresponds to a word in the dictionary. The value in each cell is the count of the word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'address': 1, 'advancements': 2, 'ambiguity': 3, 'among': 4, 'analysis': 5, 'and': 6, 'applications': 7, 'are': 8, 'as': 9, 'between': 10, 'both': 11, 'challenges': 12, 'computers': 13, 'context': 14, 'continually': 15, 'dealing': 16, 'deep': 17, 'despite': 18, 'developing': 19, 'enable': 20, 'fascinating': 21, 'field': 22, 'generate': 23, 'generation': 24, 'goal': 25, 'handling': 26, 'has': 27, 'human': 28, 'humans': 29, 'improve': 30, 'improvements': 31, 'in': 32, 'include': 33, 'interaction': 34, 'interpret': 35, 'involves': 36, 'is': 37, 'language': 38, 'languages': 39, 'leading': 40, 'learning': 41, 'lemmatization': 42, 'like': 43, 'low-resource': 44, 'machine': 45, 'many': 46, 'meaningful': 47, 'methods': 48, 'modeling': 49, 'models': 50, 'natural': 51, 'new': 52, 'nlp': 53, 'of': 54, 'open': 55, 'others': 56, 'part-of-speech': 57, 'performance': 58, 'problems': 59, 'processing': 60, 'recent': 61, 'recognition': 62, 'researchers': 63, 'revolutionized': 64, 'sentiment': 65, 'significant': 66, 'speech': 67, 'stemming': 68, 'still': 69, 'study': 70, 'such': 71, 'systems': 72, 'tagging': 73, 'tasks': 74, 'techniques': 75, 'text': 76, 'that': 77, 'the': 78, 'there': 79, 'these': 80, 'to': 81, 'tokenization': 82, 'translation': 83, 'understand': 84, 'understanding': 85, 'useful': 86, 'using': 87, 'variability': 88, 'vast': 89, 'way': 90, 'which': 91, 'with': 92, 'years': 93}\n",
      "\n",
      "Vocabulary Size: 94 | Number of Documents: 8\n",
      "\n",
      "Document 0 vector:\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "Document 0 text:\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study', 'which', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 1 vector:\n",
      "[1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "Document 1 text:\n",
      "['the', 'goal', 'of', 'nlp', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Number of documents: 8\n",
      "Vocabulary size: 94\n"
     ]
    }
   ],
   "source": [
    "def build_document_term_matrix(preprocessed_documents):\n",
    "    \"\"\"\n",
    "    Builds a Bag-of-Words document-term matrix.\n",
    "    Returns:\n",
    "    - document_term_matrix (list of lists): BoW representation of documents\n",
    "    - vocab (list): Sorted vocabulary of unique words\n",
    "    - word_to_index (dict): Mapping of word → index in vocabulary\n",
    "    \"\"\"\n",
    "    # 1. Extract unique vocabulary from the dataset and sort it\n",
    "    vocab = sorted(set(word for doc in preprocessed_documents for word in doc))\n",
    "\n",
    "    # 2. Create a word-to-index mapping\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    print(word_to_index)\n",
    "\n",
    "    # 3. Convert each document into a vector\n",
    "    document_term_matrix = []\n",
    "    \n",
    "    for tokens in preprocessed_documents:\n",
    "        doc_vector = [0] * len(vocab)  # Initialize a vector of zeros\n",
    "        for token in tokens:\n",
    "            if token in word_to_index:\n",
    "                doc_vector[word_to_index[token]] += 1  # Increment word count\n",
    "        document_term_matrix.append(doc_vector)\n",
    "\n",
    "    print(f\"\\nVocabulary Size: {len(vocab)} | Number of Documents: {len(document_term_matrix)}\\n\")\n",
    "    # Print vectors for first two sentences \n",
    "    for i, vector in enumerate(document_term_matrix[:2]):\n",
    "        print(f\"Document {i} vector:\\n{vector}\\n\")\n",
    "        print(f\"Document {i} text:\\n{preprocessed_documents[i]}\\n\\n{'#'*50}\\n\")\n",
    "\n",
    "    # Optionally, we can inspect the shape of our document-term matrix\n",
    "    print(f\"Number of documents: {len(document_term_matrix)}\")\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    return document_term_matrix, vocab, word_to_index\n",
    "\n",
    "document_term_matrix, vocab, word_to_index = build_document_term_matrix(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in this process is to build our vocabulary from our corpus - just as we have done before. Then, we assign a unique index to each word in the vocabulary. This index will be the column index in our document term matrix - corresponding to the word in the dictionary. \n",
    "\n",
    "With this in place, we can loop through our documents and count the frequency of each word in the document.  In the output above, we see that document 0 gets this vector:\n",
    "\n",
    "```python\n",
    "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "From this text\n",
    "\n",
    "```python\n",
    "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study', 'which', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language']\n",
    "```\n",
    "\n",
    "We word \"a\" has index 1 in our vocabulary, and it occurs once in the document, so we get a 1 in the vector at index 0. The word \"and\" has index 6 in our vocabulary, and it occurs once in the document, so we get a 1 in the vector at index 6. As so on.\n",
    "\n",
    "What we have effectively done now is to transform our text data into a format that can be used by machine learning models. We have transformed our text data into a numerical representation. This is the essence of the bag-of-words model. \n",
    "\n",
    "When we encounter new texts, we can use the vocabulary we built and the word_to_index mapping to transform the new text into a vector representation. \n",
    "\n",
    "**Note**: You might also consider that we need to have a mapping for each word in the vocabulary. As there are quite a few words, the amount of memory needed to store the vocabulary can explode for large datasets. In practice, we often use a fixed vocabulary size and map all other words to an unknown token. This is a simple way to handle the issue of large vocabularies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a feel for our vocabulary by looking at the most common words in our corpus. This can be useful for understanding the content of our corpus and for identifying words that might be irrelevant for our analysis - so-called stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_top_n_tokens(n, document_term_matrix):\n",
    "\n",
    "    # Sum the occurrences of each word across all documents\n",
    "    word_counts = np.sum(document_term_matrix, axis=0)\n",
    "\n",
    "    # Get the indices of the top n most common words\n",
    "    top_n_indices = np.argsort(word_counts)[-n:]\n",
    "\n",
    "    # Get the corresponding words and their counts\n",
    "    top_n_words = [vocab[i] for i in top_n_indices]\n",
    "    top_n_counts = word_counts[top_n_indices]\n",
    "\n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_n_words, top_n_counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.title(f'Top {n} Most Common Words in Document Term Matrix')\n",
    "    plt.show()\n",
    "\n",
    "visualize_top_n_tokens(10, document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 2. BoW using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we build BoW models using Scikit-Learn's `CountVectorizer`. CountVectorizer does  preprocessing and builds word vectors, using `fit_transform`, as we will see below. Also, it provides a elaborate set of options for preprocessing. You can read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load the data\n",
    "\n",
    "We will be working with the AG News dataset, which is a collection of news articles from the AG's corpus of news articles on the web. The dataset has four categories: World, Sports, Business, and Science/Technology. We will use a subset of the dataset for this example.\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 2) (7600, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "\n",
    "train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 2), (760, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac : float = 1e-2, label_map : dict[int, str] = label_map, seed : int = 42) -> pd.DataFrame:\n",
    "    \"\"\" Preprocess the dataset \n",
    "\n",
    "    Operations:\n",
    "    - Map the label to the corresponding category\n",
    "    - Filter out the labels not in the label_map\n",
    "    - Sample a fraction of the dataset (stratified by label)\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to preprocess\n",
    "    - frac (float): The fraction of the dataset to sample in each category\n",
    "    - label_map (dict): A mapping of the original label to the new label\n",
    "    - seed (int): The random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The preprocessed dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "del train\n",
    "del test\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Split the data\n",
    "\n",
    "The AG News dataset is already split into training and test sets. We will use Scikit-Learn's `train_test_split` to split the training set into a training and validation set. We will use the training set for training our BoW model and the validation set for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,) (240,) (960,) (240,)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    \n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val\n",
    "\n",
    ") = train_test_split(train_df[\"text\"], train_df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Build the BoW model\n",
    "\n",
    "All the word we did in section 1 is done by `CountVectorizer`. We will use the `fit_transform` method to build the BoW model. We will also use the `transform` method to transform the validation set into a BoW representation.\n",
    "\n",
    "**Note**: We will __not__ fit our `CountVectorizer` on the validation set. We only fit it on the training set. This is important to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countvectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_vectorized = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the shape of the BoW representation. The number of columns in the BoW representation corresponds to the number of unique words in the training set. This is the size of our vocabulary.\n",
    "\n",
    "Note that the CountVectorizer has a pretty ruthless tokenizer. It will not only remove all punctuation and lowercase all text by default, as we did, it will also remove all numbers and indeed all tokens that are not pure text. We can change this behavior by passing a custom tokenizer to the CountVectorizer, if we so desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create a classifier\n",
    "\n",
    "Below we create a simple classifier using Scikit-Learn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). We train the classifier on the BoW representation of the training set and evaluate it on the BoW representation of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression() # Note that we can set hyperparameters here\n",
    "\n",
    "lr_clf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_vectorized = cv.transform(X_val) # note that we use transform here, not fit_transform\n",
    "\n",
    "y_pred = lr_clf.predict(X_val_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Evaluate BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       1.00      1.00      1.00       238\n",
      "      Sports       1.00      1.00      1.00       240\n",
      "    Business       1.00      1.00      1.00       240\n",
      "    Sci/Tech       1.00      1.00      1.00       242\n",
      "\n",
      "    accuracy                           1.00       960\n",
      "   macro avg       1.00      1.00      1.00       960\n",
      "weighted avg       1.00      1.00      1.00       960\n",
      "\n",
      "Performance on the validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.76      0.68      0.72        62\n",
      "      Sports       0.69      0.60      0.64        60\n",
      "    Business       0.79      0.87      0.83        60\n",
      "    Sci/Tech       0.78      0.90      0.83        58\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.75      0.76      0.75       240\n",
      "weighted avg       0.75      0.76      0.75       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Performance on the training set:\")\n",
    "print(classification_report(y_train, lr_clf.predict(X_train_vectorized), target_names=label_map.values()))\n",
    "\n",
    "print(\"Performance on the validation set:\")\n",
    "print(classification_report(y_val, y_pred, target_names=label_map.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.74      0.72      0.73       190\n",
      "      Sports       0.75      0.72      0.73       190\n",
      "    Business       0.83      0.88      0.86       190\n",
      "    Sci/Tech       0.79      0.79      0.79       190\n",
      "\n",
      "    accuracy                           0.78       760\n",
      "   macro avg       0.78      0.78      0.78       760\n",
      "weighted avg       0.78      0.78      0.78       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_vectorized = cv.transform(test_df[\"text\"])\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(classification_report(test_df[\"label\"], lr_clf.predict(test_df_vectorized), target_names=label_map.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
